# CRAMMING: TRAINING A LANGUAGE MODEL ON A SINGLE GPU IN ONE DAY

*University of Maryland* investigated the downstream performance achievable with a transformer-based language model trained completely from scratch with MLM for a single day on a single consumer GPU. The paper addressed the question "How far can we get with a single GPU in just one day?"

## Annotation standards
游리 - The important research or study with respect to the core concept explained in the paper are highlighted in yellow 游리 color.

游릭 - The important summary are highlighted in green 游릭 color

游댮 - very important takeaways

游댮 + Underline - mini conlusions/study topics or crisp summary found in the paper are underlined

## Major learnings from paper

